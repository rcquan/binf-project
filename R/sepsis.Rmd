---
title: "Predicting Sepsis in the ICU"
author: Ryan Quan and Frank Chen
date: "April 30, 2015"
bibliography: "../docs/references/binf-project.bib"
csl: "../docs/references/csl/bmj.csl"
output: html_document
---

```{r dependencies, message=FALSE, echo=FALSE, warning=FALSE}
# devtools::install_github("cboettig/knitcitations")
library(knitcitations)
library(knitr)
library(Hmisc)
library(reshape2)
library(stringr)
library(magrittr)
library(lubridate)
library(ggplot2)
library(caret)
library(ROCR)
library(plyr)
library(dplyr)
```

```{r global_options, include=FALSE}
cleanbib()
options("citation_format"="pandoc")
opts_knit$set(cache.extra=rand_seed, message=FALSE, warning=FALSE, echo=FALSE)
```

# Research Question

This study will focus on developing a prediction model for the onset of sepsis in the ICU using clinical history, demographic information, and non-invasive physiological data obtained within the first hours following ICU admission. Using a large dataset of patients, routine clinical measurements obtained during initial stages of care, imputation techniques, and data mining methodologies, the goal will not only be to build a classifier that surpasses the performance of current sepsis models, but also to provide a potential early warning sysetem for sepsis in the general clinical care setting.

# Introduction

Sepsis is systemic inflammatory response syndrome (SIRS), secondary to a documented infection. Sepsis can present itself on a continuum that ranges from sepsis, severe sepsis, and septic shock, resulting in multiple organ dysfunction. The symptoms of sepsis are often non-specific and involve difficulty breathing, hypoxemia, hypoperfusion, and hypotension [@Levy2003IntensiveCareMed]. 

Although sepsis is a common condition worldwide, the current understanding of the pathophysiology of sepsis has increased substantially, and sepsis mortality has declined in the last two decades [@Stevenson2014CritCareMed]. The reason for the decline may be attributed to improved supportive care and the inherent symptomatology of patients who fall prey to sepsis. On the contrary, epidemiologic data suggests that sepsis incidence is increasing [@Stevenson2014CritCareMed]. New treatments and therapies have failed to demonstrate efficacy. Sepsis affects approximately 700,000 people per year, and accounts for approximately 200,000 deaths per year in the United States [@Hartog2009IntensiveCareMed], amassing an annual cost of 16.7 billion dollars [@Carrigan2004ClinChem].

The best form of treatment is preventive treatment. Early diagnosis and appropriate therapy must be typically be delivered before laboratory test results are known, which bases the diagnosis on the co-presence of routine clinical measures. The SIRS criteria was developed in the 1991 International Sepsis Definition Conference to address these concerns and is still commonly used in the clinical care setting to flag patients for risk of sepsis [@Levy2003IntensiveCareMed]. Patients who meet the SIRS criteria exhibit two or more of the following symptoms:

* Temperature > 38 degrees Celsius or < than 36 degrees Celsius
* Heart Rate > 90 bpm
* Respiratory Rate > than 20 or PaCO2 < 32 mm Hg
* White Blood Cell Count > 12,000/mm^3, < 4,000/mm^3, or > 10% bands

Unfortunately, the SIRS criteria has low discriminatory power in the intensive care unit as many critically ill patients who are not at risk for sepsis may also exhibit similar symptoms [@Martin2012ExpertRevAntiInfectTher]. Previous studies demonstrated the poor utility of the SIRS criteria in identifying septic patients within a clinical care setting, in which SIRS exhibited both low sensitivity and specificity [@Jaimes2003IntensiveCareMed]. In the case of identifying patients at risk for sepsis, a test with poor sensitivity can be particularly harmful as false negatives may not receive the proper prophylactic care needed to prevent sepsis-related complications. As such, a high-recall prediction model (low false negatives) to identify patients with sepsis may provide benefits to caregivers in the form of an early warning system.

While previous studies have largely focused on predicting septic shock [@Ho2014ACMTransManageInfSyst], few studies have focused on predicting earlier stages of the sepsis continuum. Multivariate logistic regression (Shavdia, 2007), decision trees [@Thiel2010JHospMed], and Dynamic Bayesian Networks [@Gultepe2014JAmMedInformAssoc] approaches have been used to predict sepsis in the intensive care unit. However, these studies tended to use a large number of invasive measurements - such as arterial blood pressure - in their feature set, reducing generalizability. Moreover, while other studies looked at the last measurements taken before the onset of sepsis [@Tang2010PhysiolMeas], few models incorporated summary statistics (mean/sd or other pairs) of clinical features in the feature set to capture the centrality and dispersion of these measurements over time. Our study attempts to synthesize and add to previous approaches by applying: a) "modern" classification methods (naive bayes, regularized logistic regression, and random forest) to potentially improve model performance, b) summary statistics to routine, non-invasive clinical features to capture information from time-series data, and c) imputation methods to avoid pitfalls due to missing data.

# Materials and Methods

## Dataset

The data was obtained from the Multiparameter Intelligent Monitoring in Intensive Care Database (MIMIC II), a semi-public database which presents ICU patient records for approximately 25,000 adults at Boston's Beth Israel Deaconess Medical Center. As a large, diverse dataset of ICU patients, MIMIC II is appropriate for building prediction models for critically ill patient populations.

Data for the analysis was sourced from a MIMIC II database instance running the PostgreSQL (version 9.2.10) engine. The package `RPostgreSQL` provided a Database Interface (DBI) compliant driver for R to access the PostgreSQL database system.

```{r database_connection, message=FALSE, echo=TRUE}
# devtools::add_path('~/usr/local/bin/psql')
# devtools::install("~/Downloads/rpostgresql-read-only/RPostgreSQL/")
db <- src_postgres(dbname = "MIMIC2",
                  host = "ec2-54-163-173-71.compute-1.amazonaws.com",
                  port = 5432,
                  user = "ec2-user",
                  password = "thisisalongpassword1234")
```

## Patient Selection

This study examined adults ($\geq$ 16 years of age). Since our objective is to train a prediction model that will detect the onset of sepsis within the first few hours of admittance to the ICU, we included only patients who were admitted to the ICU for the first sequence of their hospital visit. To avoid bias introduced by censorship, we excluded samples who have not been in the ICU for longer than 24 hours, as patients will not have accrued enough data to make a risk assessment. 

The outcome measure for this study was any instance of sepsis on the "sepsis continuum" - sepsis (995.91), severe sepsis (995.92), or septic shock (785.52) - as defined by International Classification of Diseases, 9th revision (ICD9). Severity of sepsis was not graded in this study, and thus any patient with sepsis-related ICD-9 codes was determined to have the same level of risk.

```{r caseid, cache=TRUE}
query <- sql(
"SELECT subject_id, hadm_id, icustay_id
FROM mimic2v26.icustay_detail
WHERE (icustay_first_flg = 'Y' AND subject_icustay_seq = '1' AND icustay_age_group = 'adult')
AND subject_id IN
    (SELECT DISTINCT subject_id
    FROM mimic2v26.icd9
    WHERE (code LIKE '995.9%' OR code = '785.52'))"
)
caseID <- tbl(db, query) %>%
    collect() %>%
    mutate(label = "case")
```

For our negative controls, we randomly sampled 5000 patients from the MIMIC II database who met the same exclusionary criteria.

```{r controlid, cache=TRUE}
query <- sql(
"SELECT subject_id, hadm_id, icustay_id
FROM mimic2v26.icustay_detail
WHERE (icustay_first_flg = 'Y' AND subject_icustay_seq = '1' AND icustay_age_group = 'adult')"
)
controlID <- tbl(db, query) %>%
    filter(!(subject_id %in% caseID$subject_id)) %>%
    collect() %>%
    mutate(label = "control")
```

```{r random_subset, cache=TRUE}
set.seed(1)
randomSubset <- sample(controlID$subject_id, 5000)
controls <- controlID[controlID$subject_id %in% randomSubset, ]
ids <- rbind(caseID, controls)
```

## Confounding Medical Interventions

The "ground truth" outcome for a patient is obscured by confounding medical interventions [@Paxton2013AMIAAnnuSympProc]. For example, we cannot determine the true sepsis risk for patients who receive antibiotics upon admission into the ICU because the treatment masks the "ground truth" - we don't know whether the physician was right or wrong his risk assessment of the patient. To avoid bias introduced by these so-called confounding medical interventions, patients who have undergone treatment with antibiotics within the first 24 hours will also be excluded. 

```{r microbiology, cache=TRUE}
query <- sql(
"SELECT a.subject_id AS subject_id, a.hadm_id AS hadm_id, a.ab_itemid AS ab_itemid, 
        a.charttime - b.admit_dt AS sepsis_flag
FROM mimic2v26.microbiologyevents AS a
JOIN mimic2v26.admissions AS b ON a.subject_id = b.subject_id
AND a.hadm_id = b.hadm_id"
)
microbiology <- tbl(db, query) %>%
    filter(hadm_id %in% ids$hadm_id) %>%
    filter(sepsis_flag < "1 day") %>%
    group_by(subject_id, hadm_id) %>%
    summarise(sepsis_flag = n()) %>%
    collect()
```

```{r microbiology_exclusion}
ids %<>% filter(!(hadm_id %in% microbiology$hadm_id))
```

## Feature Selection

Since we are interested in creating a classifier that can predict the onset of sepsis using accessible clinical data available within the first few hours since admission, we restrict our feature set to the `demographic`, `icustay_details`, and `chartevents` tables in the MIMIC II database, which includes:

* demographic data (gender, age, etc.)
* chart events (SOFA score, SAPS-I score)
* basic health data (height, weight, etc.)

### Demographic

We extract all variables from the `demographic` table, which includes items like religious affiliation, insurance information, and marriage status. From the `icustay_detail` table, we extract patient information that was recorded upon admission, such as age, admission time, weight, height, etc.

```{r demographic, cache=TRUE}
query <- sql(
"SELECT *
FROM mimic2v26.demographic_detail AS a
JOIN mimic2v26.icustay_detail AS b ON a.subject_id=b.subject_id
AND a.hadm_id=b.hadm_id"
)
demographic <- tbl(db, query) %>%
    filter(icustay_id %in% ids$icustay_id) %>%
    select(icustay_id, gender, icustay_intime, icustay_admit_age, contains("descr"), contains("first")) %>%
    collect()
```

To avoid the pitfalls of categorical features with zero variance, we collapsed classes into a smaller number of sensible categories. To deal timestamps, we factorized time into "morning", "afternoon", "evening", and "night" levels.

```{r}
collapseRace <- function(race) {
    if (str_detect(race, "white")) return("white")
    else if (str_detect(race, "black")) return("black")
    else if (str_detect(race, "asian")) return("asian")
    else if (str_detect(race, "hispanic")) return("hispanic")
    else if (str_detect(race, "unable") | str_detect(race, "unknown")) return("unknown")
    else return("other")
}

collapseReligion <- function(religion) {
    if (religion %in% c("buddhist", "catholic", "jewish", "other", "unobtainable", "not specified")) {return(religion)}
    else if (str_detect(religion, "orth")) {return("orthodox")}
    else if (religion %in% "hebrew") {return("jewish")}
    else if (religion %in% c("hindu", "muslim", "jehovah's witness")) {return("other")}
    else {return("protestant")}
}

demographic %<>%
    ## remove classes with zero variance
    dplyr::select(-hospital_first_flg, -icustay_first_flg) %>%
    ## sanity check for age
    filter(icustay_admit_age < 120) %>%
    ## factorize time
    mutate(icustay_intime = ifelse(hour(icustay_intime) %in% seq(6, 11), "morning",
                            ifelse(hour(icustay_intime) %in% seq(12, 17), "afternoon",
                            ifelse(hour(icustay_intime) %in% seq(18, 23), "evening", "night"))))
## missing demographic information should not be imputed
demographic <- demographic[complete.cases(demographic), ]
## collapse marital status levels
demographic <- demographic[-grep("UNKNOWN", demographic$marital_status_descr), ]
demographic$marital_status_descr <- ifelse(demographic$marital_status_descr == "SEPARATED",
                                           "divorced", tolower(demographic$marital_status_descr))
## collapse race levels
demographic$ethnicity_descr <- sapply(tolower(demographic$ethnicity_descr), 
                                      collapseRace, 
                                      USE.NAMES=FALSE)
## collapse religion levels
demographic$religion_descr <- sapply(tolower(demographic$religion_descr), 
                                      collapseReligion, 
                                      USE.NAMES=FALSE)
```

### Chart Events

We determined "routine clinical measurements" to be variables in which over 80% of our population had at least 1 clinical measurement recorded on a per hourly basis during the first 24 hours. Not surprisingly, these variables were `respiratory rate`, `pulse oximetry`, `heart rate`, `non-invasive blood pressure`, `white blood cell count`, `sofa score`, `sapsi score`, and `temperature`. 

```{r chartevents, cache=TRUE}
query <- sql(
"SELECT subject_id, icustay_id, itemid, charttime, value1num 
FROM mimic2v26.chartevents 
WHERE itemid IN (618, 646, 677, 52, 456, 211, 20002, 1542, 677)"
)

chartevents <- tbl(db, query) %>% 
	filter(icustay_id %in% ids$icustay_id) %>%
    ## hour and minutes are lost if not coerced
	mutate(timeString = as.character(charttime)) %>% 
	select(-charttime) %>% 
	collect() %>%
    mutate(charttime = ymd_hms(timeString),
           timeString = NULL) %>%
	group_by(subject_id, icustay_id, itemid) %>%
    ## set reference time and discretize by the hour
	mutate(groundTime = min(charttime),
		   intHour = as.numeric(difftime(charttime, groundTime, units="hours")), 
		   intHourR = round(intHour)) %>%
    group_by(subject_id, icustay_id, itemid, intHourR) %>%
	summarize(value = mean(value1num)) %>%
    ungroup()
```

```{r chartlabels}
query <- sql(
"SELECT itemid, label
FROM mimic2v26.d_chartitems
WHERE itemid IN (618, 646, 677, 52, 456, 211, 20002, 1542, 677)"
)
chartLabels <- tbl(db, query) %>%
    collect()
```

```{r}
chartLabels %<>%
    ## clean up formatting of labels
    mutate(label = tolower(label),
           label = gsub(" ", "_", label),
           label = ifelse(label == "temperature_c_(calc)", "temperature", label)) %>%
    ## empty variables
    filter(!(label %in% c("respiratory_sofa_score", "arterial_bp_mean")))
charteventsDiscrete <- chartevents %>%
    join(chartLabels, by="itemid", type="inner")
```

Subsequent tables from the MIMIC II database were linked through a combination of the ICU stay IDs, subject IDs, and chart item IDs (for features). Routine clinical features were then discretized by time cutoffs, melted, and casted in order to create each hour time point as a feature in itself. To evaluate the performance of our prediction models with respect to time, we then created summary statistics (mean, min, max, sd) for each clinical feature using data aggregated at 1 hour, 3 hours, 6 hours, 12 hours, and 24 hours.

```{r chartevents2, cache=TRUE}
summarizeByHour <- function(features, hour) {
    summarizedFeatures <- features %>%
        dplyr::select(icustay_id, label, intHourR, value) %>%
        filter(intHourR >= 0 & intHourR <= hour) %>%
        group_by(icustay_id, label) %>%
        dplyr::summarize(mean = mean(value, na.rm=TRUE),
                  min = min(value, na.rm=TRUE),
                  max = max(value, na.rm=TRUE),
                  std = sd(value, na.rm=TRUE)) %>%
        mutate(std = ifelse(is.na(std), 0, std)) %>%
        melt(id.vars=c("icustay_id", "label")) %>%
        dcast(icustay_id ~ label + variable, value.var="value")
    return(summarizedFeatures)
}
hours <- c(1, 3, 6, 12, 24)
featuresList <- lapply(hours, function(hour) {
    summarizeByHour(charteventsDiscrete, hour=hour)
    })
```

# Statistical Analysis

Following data extraction, 38 features were available for predictive analysis. 

Missing values were subsequently identified and imputed using kNN imputation with $k=10$. Standardization of continuous clinical measures was performed on numeric features in order to establish comparability (mean of 0 and a standard deviation of 1). Values falling outside the range will allow us to determine outliers and impossible values.

Six models were then selected for prediction of sepsis: logistic regression, regularized logistic regression, naïve Bayes, and C4.5-like decision trees (information gain), recursive partitioning trees (gini impurity), and random forest. Examiniation for collinearity was peformed using linear correlations, resulting in 27 features. The remaining models were conducted on the full set of 39 features. All methodologies were evaluated using repeated 10-fold cross validation to obtain 30 resamples with results represented using area under the receiver operating characteristic curves (AUROC).

```{r preprocessing}
preProcessFeatures <- function(features) {
    ## remove id and replace as index
    row.names(features) <- features$icustay_id
    features <- features[ , -grep("icustay_id", names(features))]
    ## split into training/testing
    set.seed(1)
    inTrain <- createDataPartition(features$label, times=1, p=0.7, list=FALSE)
    trainingUnscaled <- features[inTrain, ]
    testingUnscaled <- features[-inTrain, ]
    ## split into continous/categorical for preprocessing
    continuousVars <- names(which(sapply(features, is.numeric)))
    categoricalVars <- names(features)[!(names(features) %in% continuousVars)]
    ## fit preprocessing method on training, then apply to testing
    preProcessed <- preProcess(trainingUnscaled[, continuousVars],
                               method=c("center", "scale", "knnImpute"),
                               na.remove=TRUE,
                               k=10)
    continuousTrain <- predict(preProcessed, trainingUnscaled[, continuousVars])   
    continuousTest <- predict(preProcessed, testingUnscaled[, continuousVars])
    ## convert character into factors
    categoricalTrain <- sapply(trainingUnscaled[, categoricalVars], as.factor)
    categoricalTest <- sapply(testingUnscaled[, categoricalVars], as.factor)
    ## recombine final training/testing sets
    training <- cbind(continuousTrain, categoricalTrain)
    testing <- cbind(continuousTest, categoricalTest)
    ## remove highly correlated variables for logistic regression
    sepsisCor <- cor(continuousTrain)
    highlyCor <- findCorrelation(sepsisCor, cutoff=0.75)
    logRegTrain <- cbind(continuousTrain[, -highlyCor], categoricalTrain)
    logRegTest <- cbind(continuousTest[, -highlyCor], categoricalTest)
    
    return(list(training=training, 
                testing=testing, 
                testingUnscaled=testingUnscaled,
                logRegTrain=logRegTrain, 
                logRegTest=logRegTest))
}
featuresList <- lapply(featuresList, join, demographic, by="icustay_id", type="right")
featuresList <- lapply(featuresList, join, ids[, c("icustay_id", "label")])
## save for table1
descriptives <- featuresList[[5]]
featuresList <- lapply(featuresList, preProcessFeatures)
```

# Results

A total of 2,783 patients were used for analysis, representing 2,783 unique ICU admissions. Among the cohort, 17.8% developed some form of sepis during their ICU stay.

## Table 1: Descriptive Statistics

```{r}
describe(descriptives, digits=3, spacing=0)
```

```{r exploratory_plots, eval=FALSE}
numSum <- function(featureSet) {
	featureSet %>% 
		group_by(label) %>%
		summarise_each(funs(min(.), max(.), mean(.), median(.), sd(.)), 
					   matches("mean|age|weig|sapsi|sofa"))
}

catSum <- function(featureSet, cat) {
	return(featureSet %>% group_by_(cat, "label") %>% 
		   	summarise(count=n()) %>% ungroup %>% 
		   	group_by_("label") %>% mutate(perc=count/sum(count)))
}

loopDescrPlot <- function(df) {
	ggplot(df, aes_string(colnames(df)[1], "perc")) +
		geom_point(aes(color=as.factor(label), size=perc)) +
		ylab("Percent") + 
		xlab("Trait") + 
		coord_cartesian(xlim=c(0, 1)) +
		ggtitle(colnames(df)[1]) +
		scale_color_manual(values = c("red", "blue")) +
		theme_bw() +
		coord_flip()
}

catVars <- as.list(colnames(features)[c(2, 5:14)])
catStats <- lapply(catVars, catSum, featureSet=features)
catPlots <- lapply(catStats[c(1:7, 10:11)], loopDescrPlot)
catPlots
numSum(features)
```

## Cross Validation

```{r}
fitModels <- function(features) {
    library(doMC)
    registerDoMC(cores=2)
    
    training <- features$training
    logRegTrain <- features$logRegTrain
    
	tc <- trainControl("repeatedcv", 
	                   number=10, 
	                   repeats=3,
	                   classProbs=TRUE,
	                   savePredictions=TRUE,
	                   summaryFunction=twoClassSummary)
	tcNoParallel <- trainControl("repeatedcv", 
	                   number=10, 
	                   repeats=3,
	                   classProbs=TRUE,
	                   savePredictions=TRUE,
	                   summaryFunction=twoClassSummary,
	                   allowParallel=FALSE)
	
	logModel <- train(label ~ ., 
	                  data=logRegTrain, 
	                  trControl=tc,  
	                  method="glm", 
	                  metric="ROC", 
	                  family="binomial")
	message("Finished fitting Logistic Regression.")
	
	regLogModel <- train(label ~ ., 
	                  data=training, 
	                  trControl=tc,  
	                  method="glmnet", 
	                  metric="ROC", 
	                  family="binomial",
	                  tuneLength=5)
	message("Finished fitting Regularized Logistic Regression.")

	nbModel <- train(label ~ ., 
	                    data=training, 
	                    trControl=tcNoParallel, 
						method="nb", 
						metric="ROC",
						tuneLength=5)  
	message("Finished fitting Naive Bayes.")

	infoModel <- train(label ~ ., 
	                   data=training, 
	                   trControl=tcNoParallel,
					   method="J48", 
					   metric="ROC",
					   tuneLength=10) 
	message("Finished fitting Decision Tree (Info Gain).")
	
	giniModel <- train(label ~ ., 
	                   data=training, 
	                   trControl=tc, 
					   method="rpart2",
					   metric="ROC",
					   tuneLength=10) 
	message("Finished fitting Decision Tree (Gini Impurity).")
	
	rfModel <- train(label ~ ., 
	                 data=training, 
	                 trControl=tc,
					 method="rf", 
					 metric="ROC",
					 tuneLength=3)
	message("Finished fitting Random Forest.")
	
	return(list(logModel, regLogModel, nbModel, giniModel, infoModel, rfModel))
}
modelList <- lapply(featuresList, fitModels)
```

```{r}
res <- resamples(modelList)
summary(res)
dotplot(res)
```


```{r}
library(ROCR)
probsInfo <- predict(modelList$randomForest, testing, type="prob")
pred <- prediction(1-probsInfo$case, testing$label)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
```

```{r}
finalModel <- 
probsInfo <- predict(finalModel, testing, type="prob")
rocCurve <- roc(response=testing$label,
                predictor=probsInfo[, "case"],
                levels=rev(levels(testing$label)))
plot(rocCurve, print.thres=c(.5, .2), type="S", legacy.axes = TRUE)
```


# Test Predictions

```{r baseline}
SIRS <- function(temp, hr, rr, wbc) {
    count <- 0
    ## better safe than sorry
    if (is.na(temp) | is.na(hr) | is.na(rr) | is.na(wbc)) return("case")
    if (temp > 38 | temp < 36) count <- count + 1
    if (hr > 90) count <- count + 1
    if (rr > 20) count <- count + 1
    if (wbc > 12) count <- count + 1
    return(ifelse(count >= 2, "case", "control"))
}

baselinePred <- sapply(1:nrow(testing1), function(i) {
    SIRS(testing1$temperature_mean[i], testing1$heart_rate_mean[i], testing1$respiratory_rate_mean[i], testing1$wbc_mean[i])
    })

confusionMatrix(baselinePred, testing1$label)
confusionMatrix(rep("control", nrow(testing1)), testing1$label)

```


# SIRS Classifier

```{r}



```



