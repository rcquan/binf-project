---
title: "Predicting Sepsis in the ICU"
author: "Ryan Quan and Frank Chen"
date: "May 11, 2015"
output: pdf_document
csl: ../docs/references/csl/bmj.csl
bibliography: ../docs/references/binf-project.bib
---

```{r dependencies, message=FALSE, echo=FALSE, warning=FALSE}
# devtools::install_github("cboettig/knitcitations")
library(knitcitations)
library(knitr)
library(Hmisc)
library(reshape2)
library(stringr)
library(magrittr)
library(lubridate)
library(ggplot2)
library(RColorBrewer)
library(caret)
library(ROCR)
library(plyr)
library(dplyr)
```

```{r global_options, include=FALSE}
cleanbib()
options("citation_format"="pandoc")
opts_knit$set(cache.extra=rand_seed)
opts_chunk$set(message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE)
```

# Research Question

This study will focus on developing a prediction model for the onset of sepsis in the ICU using clinical history, demographic information, and non-invasive physiological data obtained within the first hours following ICU admission. Using a large dataset of patients, routine clinical measurements obtained during initial stages of care, imputation techniques, and data mining methodologies, the goal will not only be to build a classifier that surpasses the performance of current sepsis models, but also to provide a potential early warning sysetem for sepsis in the general clinical care setting.

# Introduction

Sepsis is systemic inflammatory response syndrome (SIRS), secondary to a documented infection. Sepsis can present itself on a continuum that ranges from sepsis, severe sepsis, and septic shock, resulting in multiple organ dysfunction. The symptoms of sepsis are often non-specific and involve difficulty breathing, hypoxemia, hypoperfusion, and hypotension [@Levy2003IntensiveCareMed]. 

Although sepsis is a common condition worldwide, the current understanding of the pathophysiology of sepsis has increased substantially, and sepsis mortality has declined in the last two decades [@Stevenson2014CritCareMed]. The reason for the decline may be attributed to improved supportive care and the inherent symptomatology of patients who fall prey to sepsis. On the contrary, epidemiologic data suggests that sepsis incidence is increasing [@Stevenson2014CritCareMed]. New treatments and therapies have failed to demonstrate efficacy. Sepsis affects approximately 700,000 people per year, and accounts for approximately 200,000 deaths per year in the United States [@Hartog2009IntensiveCareMed], amassing an annual cost of 16.7 billion dollars [@Carrigan2004ClinChem].

The best form of treatment is preventive treatment. Early diagnosis and appropriate therapy must be typically be delivered before laboratory test results are known, which bases the diagnosis on the co-presence of routine clinical measures. The SIRS criteria was developed in the 1991 International Sepsis Definition Conference to address these concerns and is still commonly used in the clinical care setting to flag patients for risk of sepsis [@Levy2003IntensiveCareMed]. Patients who meet the SIRS criteria exhibit two or more of the following symptoms:

* Temperature > 38 degrees Celsius or < than 36 degrees Celsius
* Heart Rate > 90 bpm
* Respiratory Rate > than 20 or PaCO2 < 32 mm Hg
* White Blood Cell Count > 12,000/mm^3, < 4,000/mm^3, or > 10% bands

Unfortunately, the SIRS criteria has low discriminatory power in the intensive care unit as many critically ill patients who are not at risk for sepsis may also exhibit similar symptoms [@Martin2012ExpertRevAntiInfectTher]. Previous studies demonstrated the poor utility of the SIRS criteria in identifying septic patients within a clinical care setting, in which SIRS exhibited both low sensitivity and specificity [@Jaimes2003IntensiveCareMed]. In the case of identifying patients at risk for sepsis, a test with poor sensitivity can be particularly harmful as false negatives may not receive the proper prophylactic care needed to prevent sepsis-related complications. As such, a high-recall prediction model (low false negatives) to identify patients with sepsis may provide benefits to caregivers in the form of an early warning system.

While previous studies have largely focused on predicting septic shock [@Ho2014ACMTransManageInfSyst], few studies have focused on predicting earlier stages of the sepsis continuum. Multivariate logistic regression (Shavdia, 2007), decision trees [@Thiel2010JHospMed], and Dynamic Bayesian Networks [@Gultepe2014JAmMedInformAssoc] approaches have been used to predict sepsis in the intensive care unit. However, these studies tended to use a large number of invasive measurements - such as arterial blood pressure - in their feature set, reducing generalizability. Moreover, while other studies looked at the last measurements taken before the onset of sepsis [@Tang2010PhysiolMeas], few models incorporated summary statistics (mean/sd or other pairs) of clinical features in the feature set to capture the centrality and dispersion of these measurements over time. Our study attempts to synthesize and add to previous approaches by applying: a) "modern" classification methods (naive bayes, regularized logistic regression, and random forest) to potentially improve model performance, b) summary statistics to routine, non-invasive clinical features to capture information from time-series data, and c) imputation methods to avoid pitfalls due to missing data.

# Materials and Methods

## Dataset

The data was obtained from the Multiparameter Intelligent Monitoring in Intensive Care Database (MIMIC II), a semi-public database which presents ICU patient records for approximately 25,000 adults at Boston's Beth Israel Deaconess Medical Center. As a large, diverse dataset of ICU patients, MIMIC II is appropriate for building prediction models for critically ill patient populations.

Data for the analysis was sourced from a MIMIC II database instance running the PostgreSQL (version 9.2.10) engine. The package `RPostgreSQL` provided a Database Interface (DBI) compliant driver for R to access the PostgreSQL database system.

```{r database_connection, message=FALSE, echo=TRUE, eval=FALSE}
# devtools::add_path('~/usr/local/bin/psql')
# devtools::install("~/Downloads/rpostgresql-read-only/RPostgreSQL/")
db <- src_postgres(dbname = "MIMIC2",
                  host = "ec2-54-163-173-71.compute-1.amazonaws.com",
                  port = 5432,
                  user = "ec2-user",
                  password = "thisisalongpassword1234")
```

## Patient Selection

This study examined adults ($\geq$ 16 years of age). Since our objective is to train a prediction model that will detect the onset of sepsis within the first few hours of admittance to the ICU, we included only patients who were admitted to the ICU for the first sequence of their hospital visit. To avoid bias introduced by censorship, we excluded samples who have not been in the ICU for longer than 24 hours, as patients will not have accrued enough data to make a risk assessment. 

The outcome measure for this study was any instance of sepsis on the "sepsis continuum" - sepsis (995.91), severe sepsis (995.92), or septic shock (785.52) - as defined by International Classification of Diseases, 9th revision (ICD9). Severity of sepsis was not graded in this study, and thus any patient with sepsis-related ICD-9 codes was determined to have the same level of risk.

```{r caseid, eval=FALSE}
query <- sql(
"SELECT subject_id, hadm_id, icustay_id
FROM mimic2v26.icustay_detail
WHERE (icustay_first_flg = 'Y' AND subject_icustay_seq = '1' AND icustay_age_group = 'adult')
AND subject_id IN
    (SELECT DISTINCT subject_id
    FROM mimic2v26.icd9
    WHERE (code LIKE '995.9%' OR code = '785.52'))"
)
caseID <- tbl(db, query) %>%
    collect() %>%
    mutate(label = "case")
```

For our negative controls, we randomly sampled 5000 patients from the MIMIC II database who met the same exclusionary criteria.

```{r controlid, eval=FALSE}
query <- sql(
"SELECT subject_id, hadm_id, icustay_id
FROM mimic2v26.icustay_detail
WHERE (icustay_first_flg = 'Y' AND subject_icustay_seq = '1' AND icustay_age_group = 'adult')"
)
controlID <- tbl(db, query) %>%
    filter(!(subject_id %in% caseID$subject_id)) %>%
    collect() %>%
    mutate(label = "control")
```

```{r random_subset, eval=FALSE}
set.seed(1)
randomSubset <- sample(controlID$subject_id, 5000)
controls <- controlID[controlID$subject_id %in% randomSubset, ]
ids <- rbind(caseID, controls)
```

## Confounding Medical Interventions

The "ground truth" outcome for a patient is obscured by confounding medical interventions [@Paxton2013AMIAAnnuSympProc]. For example, we cannot determine the true sepsis risk for patients who receive antibiotics upon admission into the ICU because the treatment masks the "ground truth" - we don't know whether the physician was right or wrong his risk assessment of the patient. To avoid bias introduced by these so-called confounding medical interventions, patients who have undergone treatment with antibiotics within the first 24 hours will also be excluded. 

```{r microbiology, eval=FALSE}
query <- sql(
"SELECT a.subject_id AS subject_id, a.hadm_id AS hadm_id, a.ab_itemid AS ab_itemid, 
        a.charttime - b.admit_dt AS sepsis_flag
FROM mimic2v26.microbiologyevents AS a
JOIN mimic2v26.admissions AS b ON a.subject_id = b.subject_id
AND a.hadm_id = b.hadm_id"
)
microbiology <- tbl(db, query) %>%
    filter(hadm_id %in% ids$hadm_id) %>%
    filter(sepsis_flag < "1 day") %>%
    group_by(subject_id, hadm_id) %>%
    summarise(sepsis_flag = n()) %>%
    collect()
```

```{r microbiology_exclusion, eval=FALSE}
ids %<>% filter(!(hadm_id %in% microbiology$hadm_id))
```

## Feature Selection

Since we are interested in creating a classifier that can predict the onset of sepsis using accessible clinical data available within the first few hours since admission, we restrict our feature set to the `demographic`, `icustay_details`, and `chartevents` tables in the MIMIC II database, which includes:

* demographic data (gender, age, etc.)
* chart events (SOFA score, SAPS-I score)
* basic health data (height, weight, etc.)

### Demographic

We extract all variables from the `demographic` table, which includes items like religious affiliation, insurance information, and marriage status. From the `icustay_detail` table, we extract patient information that was recorded upon admission, such as age, admission time, weight, height, etc.

```{r demographic, eval=FALSE}
query <- sql(
"SELECT *
FROM mimic2v26.demographic_detail AS a
JOIN mimic2v26.icustay_detail AS b ON a.subject_id=b.subject_id
AND a.hadm_id=b.hadm_id"
)
demographic <- tbl(db, query) %>%
    filter(icustay_id %in% ids$icustay_id) %>%
    select(icustay_id, gender, icustay_intime, icustay_admit_age, contains("descr"), contains("first")) %>%
    collect()
```

To avoid the pitfalls of categorical features with zero variance, we collapsed classes into a smaller number of sensible categories. To deal timestamps, we factorized time into "morning", "afternoon", "evening", and "night" levels.

```{r demographic_preprocess, eval=FALSE}
collapseRace <- function(race) {
    if (str_detect(race, "white")) return("white")
    else if (str_detect(race, "black")) return("black")
    else if (str_detect(race, "asian")) return("asian")
    else if (str_detect(race, "hispanic")) return("hispanic")
    else if (str_detect(race, "unable") | str_detect(race, "unknown")) return("unknown")
    else return("other")
}

collapseReligion <- function(religion) {
    if (religion %in% c("buddhist", "catholic", "jewish", "other", "unobtainable", "not specified")) {return(religion)}
    else if (str_detect(religion, "orth")) {return("orthodox")}
    else if (religion %in% "hebrew") {return("jewish")}
    else if (religion %in% c("hindu", "muslim", "jehovah's witness")) {return("other")}
    else {return("protestant")}
}

demographic %<>%
    ## remove classes with zero variance
    dplyr::select(-hospital_first_flg, -icustay_first_flg) %>%
    ## sanity check for age
    filter(icustay_admit_age < 120) %>%
    ## factorize time
    mutate(icustay_intime = ifelse(hour(icustay_intime) %in% seq(6, 11), "morning",
                            ifelse(hour(icustay_intime) %in% seq(12, 17), "afternoon",
                            ifelse(hour(icustay_intime) %in% seq(18, 23), "evening", "night"))))
## missing demographic information should not be imputed
demographic <- demographic[complete.cases(demographic), ]
## collapse marital status levels
demographic <- demographic[-grep("UNKNOWN", demographic$marital_status_descr), ]
demographic$marital_status_descr <- ifelse(demographic$marital_status_descr == "SEPARATED",
                                           "divorced", tolower(demographic$marital_status_descr))
## collapse race levels
demographic$ethnicity_descr <- sapply(tolower(demographic$ethnicity_descr), 
                                      collapseRace, 
                                      USE.NAMES=FALSE)
## collapse religion levels
demographic$religion_descr <- sapply(tolower(demographic$religion_descr), 
                                      collapseReligion, 
                                      USE.NAMES=FALSE)
```

### Chart Events

We determined "routine clinical measurements" to be variables in which over 80% of our population had at least 1 clinical measurement recorded on a per hourly basis during the first 24 hours. Not surprisingly, these variables were `respiratory rate`, `pulse oximetry`, `heart rate`, `non-invasive blood pressure`, `white blood cell count`, `sofa score`, `sapsi score`, and `temperature`. 

```{r chartevents, eval=FALSE}
query <- sql(
"SELECT subject_id, icustay_id, itemid, charttime, value1num 
FROM mimic2v26.chartevents 
WHERE itemid IN (618, 646, 677, 52, 456, 211, 20002, 1542, 677)"
)

chartevents <- tbl(db, query) %>% 
	filter(icustay_id %in% ids$icustay_id) %>%
    ## hour and minutes are lost if not coerced
	mutate(timeString = as.character(charttime)) %>% 
	select(-charttime) %>% 
	collect() %>%
    mutate(charttime = ymd_hms(timeString),
           timeString = NULL) %>%
	group_by(subject_id, icustay_id, itemid) %>%
    ## set reference time and discretize by the hour
	mutate(groundTime = min(charttime),
		   intHour = as.numeric(difftime(charttime, groundTime, units="hours")), 
		   intHourR = round(intHour)) %>%
    group_by(subject_id, icustay_id, itemid, intHourR) %>%
	summarize(value = mean(value1num)) %>%
    ungroup()
```

```{r chartlabels, eval=FALSE}
query <- sql(
"SELECT itemid, label
FROM mimic2v26.d_chartitems
WHERE itemid IN (618, 646, 677, 52, 456, 211, 20002, 1542, 677)"
)
chartLabels <- tbl(db, query) %>%
    collect()
```

```{r chartlabels_preprocess, eval=FALSE}
chartLabels %<>%
    ## clean up formatting of labels
    mutate(label = tolower(label),
           label = gsub(" ", "_", label),
           label = ifelse(label == "temperature_c_(calc)", "temperature", label)) %>%
    ## empty variables
    filter(!(label %in% c("respiratory_sofa_score", "arterial_bp_mean")))
charteventsDiscrete <- chartevents %>%
    join(chartLabels, by="itemid", type="inner")
```

Subsequent tables from the MIMIC II database were linked through a combination of the ICU stay IDs, subject IDs, and chart item IDs (for features). Routine clinical features were then discretized by time cutoffs, melted, and casted in order to create each hour time point as a feature in itself. To evaluate the performance of our prediction models with respect to time, we then created summary statistics (mean, min, max, sd) for each clinical feature using the discretized values.

```{r load_data}
load("../data/sql_dump.RData")
```

```{r discretize_chartevents}
summarizeByHour <- function(features, hour) {
    summarizedFeatures <- features %>%
        dplyr::select(icustay_id, label, intHourR, value) %>%
        filter(intHourR >= 0 & intHourR <= hour) %>%
        group_by(icustay_id, label) %>%
        dplyr::summarize(mean = mean(value, na.rm=TRUE),
                  min = min(value, na.rm=TRUE),
                  max = max(value, na.rm=TRUE),
                  std = sd(value, na.rm=TRUE)) %>%
        mutate(std = ifelse(is.na(std), 0, std)) %>%
        melt(id.vars=c("icustay_id", "label")) %>%
        dcast(icustay_id ~ label + variable, value.var="value")
    return(summarizedFeatures)
}
hours <- c(1, 3, 6, 12, 24)
featuresList <- lapply(hours, function(hour) {
    summarizeByHour(charteventsDiscrete, hour=hour)
    })
```

# Statistical Analysis

Following data extraction, 38 features were available for predictive analysis. 

Missing values were subsequently identified and imputed using kNN imputation with $k=10$. Standardization of continuous clinical measures was performed on numeric features in order to establish comparability (mean of 0 and a standard deviation of 1). Values falling outside the range will allow us to determine outliers and impossible values.

Six models were then selected for prediction of sepsis: logistic regression, regularized logistic regression, naÃ¯ve Bayes, and C4.5-like decision trees (information gain), recursive partitioning trees (gini impurity), and random forest. Examiniation for collinearity was peformed using linear correlations, resulting in 27 features. The remaining models were conducted on the full set of 39 features. All methodologies were evaluated using repeated 10-fold cross validation to obtain 30 resamples with results represented using area under the receiver operating characteristic curves (AUROC). AUROC was chosen as a performance statistic because it is insensitive to class balance issues. 

Since our goal is to detect early onset of sepsis, we ideally want our predictive model to have high AUROC when predicting on data available within the first hour. We will also train our models on data collected at the following time intervals, e.g. 3 hours, 6 hours, 12 hours, and 24 hours after ICU admission. By doing this, we will be able to assess the stability of our performance statistics with respect to elapsed time in the ICU and determine whether the first hour of data is sufficient in a predictive model for sepsis.

Finally, to determine whether or not our prediction models perform better than the SIRS criteria mentioned above, we will create an implementation of the SIRS criteria and compare models using the "balanced accuracy" measure, which avoids inflated performance estimates on imbalanced datasets. It is defined as the arithmetic mean of sensitivity and specificity:

$balanced\ accuracy = \frac{sensitivity + specificity}{2}$

```{r preprocessing, cache=TRUE}
preProcessFeatures <- function(features) {
    ## remove id and replace as index
    row.names(features) <- features$icustay_id
    features <- features[ , -grep("icustay_id", names(features))]
    ## split into training/testing
    set.seed(1)
    inTrain <- createDataPartition(features$label, times=1, p=0.7, list=FALSE)
    trainingUnscaled <- features[inTrain, ]
    testingUnscaled <- features[-inTrain, ]
    ## split into continous/categorical for preprocessing
    continuousVars <- names(which(sapply(features, is.numeric)))
    categoricalVars <- names(features)[!(names(features) %in% continuousVars)]
    ## fit preprocessing method on training, then apply to testing
    preProcessed <- preProcess(trainingUnscaled[, continuousVars],
                               method=c("center", "scale", "knnImpute"),
                               na.remove=TRUE,
                               k=10)
    continuousTrain <- predict(preProcessed, trainingUnscaled[, continuousVars])   
    continuousTest <- predict(preProcessed, testingUnscaled[, continuousVars])
    ## convert character into factors
    categoricalTrain <- sapply(trainingUnscaled[, categoricalVars], as.factor)
    categoricalTest <- sapply(testingUnscaled[, categoricalVars], as.factor)
    ## recombine final training/testing sets
    training <- cbind(continuousTrain, categoricalTrain)
    testing <- cbind(continuousTest, categoricalTest)
    ## remove highly correlated variables for logistic regression
    sepsisCor <- cor(continuousTrain)
    highlyCor <- findCorrelation(sepsisCor, cutoff=0.75)
    logRegTrain <- cbind(continuousTrain[, -highlyCor], categoricalTrain)
    logRegTest <- cbind(continuousTest[, -highlyCor], categoricalTest)
    
    return(list(training=training, 
                testing=testing, 
                testingUnscaled=testingUnscaled,
                logRegTrain=logRegTrain, 
                logRegTest=logRegTest))
}
featuresList <- lapply(featuresList, join, demographic, by="icustay_id", type="right")
featuresList <- lapply(featuresList, join, ids[, c("icustay_id", "label")])
## save for table1
descriptives <- featuresList[[5]]
featuresList <- lapply(featuresList, preProcessFeatures)
```

# Results

A total of 2,783 patients were used for analysis, representing 2,783 unique ICU admissions. Among the cohort, 17.8% developed some form of sepis during their ICU stay.

## Table 1: Descriptive Statistics for Selected Features

```{r descriptives}
descriptives %>%
    dplyr::select(label, gender, contains("descr"), contains("mean")) %>%
    describe(digits=3, spacing=0)
```

```{r exploratory_plots, eval=FALSE}
numSum <- function(featureSet) {
	featureSet %>% 
		group_by(label) %>%
		summarise_each(funs(min(.), max(.), mean(.), median(.), sd(.)), 
					   matches("mean|age|weig|sapsi|sofa"))
}

catSum <- function(featureSet, cat) {
	return(featureSet %>% group_by_(cat, "label") %>% 
		   	summarise(count=n()) %>% ungroup %>% 
		   	group_by_("label") %>% mutate(perc=count/sum(count)))
}

loopDescrPlot <- function(df) {
	ggplot(df, aes_string(colnames(df)[1], "perc")) +
		geom_point(aes(color=as.factor(label), size=perc)) +
		ylab("Percent") + 
		xlab("Trait") + 
		coord_cartesian(xlim=c(0, 1)) +
		ggtitle(colnames(df)[1]) +
		scale_color_manual(values = c("red", "blue")) +
		theme_bw() +
		coord_flip()
}

catVars <- as.list(colnames(features)[c(2, 5:14)])
catStats <- lapply(catVars, catSum, featureSet=features)
catPlots <- lapply(catStats[c(1:7, 10:11)], loopDescrPlot)
catPlots
numSum(features)
```

## Model Selection

```{r model_fit, cache=TRUE}
fitModels <- function(features) {
    library(doMC)
    registerDoMC(cores=2)
    
    training <- features$training
    logRegTrain <- features$logRegTrain
    
	tc <- trainControl("repeatedcv", 
	                   number=10, 
	                   repeats=3,
	                   classProbs=TRUE,
	                   savePredictions=TRUE,
	                   summaryFunction=twoClassSummary)
	tcNoParallel <- trainControl("repeatedcv", 
	                   number=10, 
	                   repeats=3,
	                   classProbs=TRUE,
	                   savePredictions=TRUE,
	                   summaryFunction=twoClassSummary,
	                   allowParallel=FALSE)
	
	logModel <- train(label ~ ., 
	                  data=logRegTrain, 
	                  trControl=tc,  
	                  method="glm", 
	                  metric="ROC", 
	                  family="binomial")
	message("Finished fitting Logistic Regression.")
	
	regLogModel <- train(label ~ ., 
	                  data=training, 
	                  trControl=tc,  
	                  method="glmnet", 
	                  metric="ROC", 
	                  family="binomial",
	                  tuneLength=5)
	message("Finished fitting Regularized Logistic Regression.")

	nbModel <- train(label ~ ., 
	                    data=training, 
	                    trControl=tcNoParallel, 
						method="nb", 
						metric="ROC",
						tuneLength=5)  
	message("Finished fitting Naive Bayes.")

	infoModel <- train(label ~ ., 
	                   data=training, 
	                   trControl=tcNoParallel,
					   method="J48", 
					   metric="ROC",
					   tuneLength=10) 
	message("Finished fitting Decision Tree (Info Gain).")
	
	giniModel <- train(label ~ ., 
	                   data=training, 
	                   trControl=tc, 
					   method="rpart2",
					   metric="ROC",
					   tuneLength=10) 
	message("Finished fitting Decision Tree (Gini Impurity).")
	
	rfModel <- train(label ~ ., 
	                 data=training, 
	                 trControl=tc,
					 method="rf", 
					 metric="ROC",
					 tuneLength=3)
	message("Finished fitting Random Forest.")
	
	return(list(logModel=logModel, 
	            regLogModel=regLogModel, 
	            nbModel=nbModel, 
	            giniModel=giniModel, 
	            infoModel=infoModel, 
	            rfModel=rfModel))
}
modelList <- lapply(featuresList, fitModels)
```

```{r model_cache}
save.image( "../data/model_cache.Rdata")
```

### Cross-Validation

Performance statistics for repeated 10-fold cross validation did not vary significantly when trained on data available at 1 hour, 3 hours, 6 hours, 12 hours, and 24 hours (shown in sequential order).

```{r resample_plots, results="hide", fig.align="center", fig.height=4, fig.width=4}
modelNames <- c("LogReg", "RegularizedLogReg", "NaiveBayes", "infoTree", "giniTree", "randomForest")
res <- lapply(modelList, resamples, modelNames=modelNames)
lapply(res, dotplot)
```

Given the results of our sensitivity analysis, we opted for the models fitted on the first hour of data made available in the ICU. Resampled ROC statistics for each of these models are shown below.

```{r roc_stats}
resam <- summary(res[[1]])
kable(resam$statistics$ROC[, -7], digits=4)
```

Under 10-fold cross validation, regularized logistic regression had an ROC of 0.808, random forest had an ROC of 0.804, logistic regression had an ROC of 0.781, Naive Bayes had an ROC of 0.764, C4.5-like decision trees had an ROC of 0.67, and CART decision trees had an ROC of 0.528. With respect to the tradeoff between performance and interpretability, logistic regression was not too shabby.

```{r model_1hr}
model <- modelList[[5]]
testing <- featuresList[[5]]$testing
logRegTest <- featuresList[[5]]$logRegTest
```

### Variable Importance

For logistic regression, top features selected for classification included `SAPS-I score`, `MICU as First Service`, `Minimum Non-invasive Blood Pressure`, and `Maximum Pulse Oximetry`. For regularized logistic regression, 10 features remained after coefficients were penalized, which included `CSRU as First Service`, `Private Medical Insurance`, and `Morning ICU Admission`. As determined by the decrease in the mean Gini in our CART model, `First SOFA score`, `Minimum White Blood Cell Count`, and `Maximium Respiratory Rate` were the most important features. As determined by the decrease in the mean Gini in our random forest model, `Mean White Blood Cell Count`, `First SOFA Socre`, and `Mean Heart Rate` were determined to be the most important features. SOFA, SAPS-I, heart rate, white blood cell count, pulse oximetry, and respiratory rate have been previously described as risk factors for sepsis (see SIRS criteria above).

```{r var_importance, eval=FALSE}
lapply(model, varImp, scale=FALSE)
```

```{r varimp_plot}
varImpPlot(model[[6]]$finalModel, main="RF Variable Importance", cex=0.7)
```

## Hold-Out Performance

### Between-Model Comparison

Most of the classifiers showed strong predictive power in predicting sepsis in the ICU. Because our positive class is in the minority (cases ~ 18%), accuracy statistics were not suitable for model assessment. Instead we used ROC statistics, which are agnostic to class imbalance issues. C4.5-like decision trees were not included in the figure below because we were unable to debug the prediction task.

```{r roc_plot, results="hide"}
testingList <- list(testing$label, testing$label, testing$label, testing$label, testing$label)
cols <- brewer.pal(5, "Set1")

logRegPred <- 1 - predict(model[[1]], logRegTest, type="prob")$case
regLogRegPred <- 1 - predict(model[[2]], testing, type="prob")$case
nbPred <- 1 - predict(model[[3]], testing, type="prob")$case
giniPred <- 1 - predict(model[[4]], testing, type="prob")$case
rfPred <- 1 - predict(model[[6]], testing, type="prob")$case

probsList <- list(logRegPred, regLogRegPred, nbPred, giniPred, rfPred)
predList <- prediction(probsList, testingList)
perf <- performance(predList, "tpr", "fpr")

plot(perf, col=as.list(cols))
legend(0.7, 0.6, modelNames[-5], col=cols, lwd=3, cex=0.6)
```

### Baseline Comparison

So did our models perform better than the current SIRS criteria in predicting sepsis in the ICU?

```{r baseline_prediction}
SIRS <- function(temp, hr, rr, wbc) {
    count <- 0
    ## better safe than sorry
    if (is.na(temp) | is.na(hr) | is.na(rr) | is.na(wbc)) return("case")
    if (temp > 38 | temp < 36) count <- count + 1
    if (hr > 90) count <- count + 1
    if (rr > 20) count <- count + 1
    if (wbc > 12) count <- count + 1
    return(ifelse(count >= 2, "case", "control"))
}
testingUnscaled <- featuresList[[1]]$testingUnscaled
baselinePred <- sapply(1:nrow(testingUnscaled), function(i) {
    SIRS(testingUnscaled$temperature_mean[i], testingUnscaled$heart_rate_mean[i], 
         testingUnscaled$respiratory_rate_mean[i], testingUnscaled$wbc_mean[i])
    })
```

```{r performance_stats}
logRegPred <- predict(model[[1]], logRegTest)
regLogRegPred <- predict(model[[2]], testing)
nbPred <- predict(model[[3]], testing)
giniPred <- predict(model[[4]], testing)
rfPred <- predict(model[[6]], testing)
predList <- list(baselinePred, logRegPred, regLogRegPred, nbPred, giniPred, rfPred)

perfStats <- lapply(predList, confusionMatrix, testing$label)
perfStats <- lapply(perfStats, with, byClass)
perfStats <- lapply(perfStats, "[", c("Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Balanced Accuracy"))
perfStats <- do.call(rbind, perfStats)
perfStats <- data.frame(perfStats)
row.names(perfStats) <- c("SIRS", "LogReg", "RegLogReg", "NaiveBayes", "GiniTree", "RandomForest")
kable(perfStats, digits=4)
```

Yes, but not by much. Moreover, training on data available at the 3 hour, 6 hour, 12 hour, and 24 hour time slices only marginally improved the performance of our prediction models with regularized logistic regression leading the pack. At the end of the day, however, logistic regression had the best balanced accuracy using one hour's worth of data on our hold-out set at 59.5%. 

# Discussion

When we began this project, we sought to answer the following question: could we develop a prediction model for the onset of sepsis in the ICU using clinical history and non-invasive physiological data obtained in the first hour of admission? The short answer: yes, but predictive performance is only marginally better than current (and simpler) methodologies.

Not suprisingly, fitting more complex models does not necessarily lead to better performance. While we used repeated 10-fold cross validation to fine-tune complexity parameters, we very likely overfit our training data using random forest - which performed worse than the SIRS criteria on our hold-out set. In some instances, regularized logistic regression performed marginally better than other models because 10-fold cross validation tuned the alpha parameter to 1 (LASSO), which allowed the model to undergo automated feature selection by shrinking some coefficients zero. However, the tradeoff in the intepretability of the model in its native form may not be worth the marginal improvements in predictive power. After all, clinical decision support systems should have some descriptive transparency. Penalized regression models and random forest are not likely to provide that. "Vanilla" logistic regression - with well-understood parameters, wide usage, and one of the top balanced accuracy measures in our study - is.

Moreover, it is important to note that performance statistics provided in this study are based on a 50% probability threshold for classification. In other words, samples are classified as "sepsis" only if the probability of class is greater than 50%. Since false positives (patients classified as sepsis when they are not at risk) and false negatives (patients classified as not sepsis when they are at risk) are not equal in this scenario, we may want to adjust the treshold downwards and attempt to improve the recall of our prediction models. 

## Limitations and Future Work

Better features would have likely improved the performance of our prediction models. For one, our decision to restrict features to routine clinical variables was based heavily on domain knowledge rather than a purely data-driven methodology. This meant that our models suffered from substantial information loss. Determining a better definition for "routine" and translating that definition into an automated feature selection process could have given us more (and better) features to work with from the start. Second, valuable information from temporal features was not fully extracted using summary statistics (mean, min, max, sd). While these simple statistical features are a good starting point, future prediction models could incoporate parameters of ARIMA models or frequencies of the $k$ peaks in amplitude in the Discrete Fourier Transforms for the detrended $d$ dimensions. 

Moreover, our decision to define positive cases of sepsis from ICD-9 codes presented a number of problems. For one, our so-called "ground truth" labels may, in fact, be mislabled due to administrative errors. This would mean that we were training our classifiers on incorrectly labeled samples, essentially making this work non-generalizable and effectively useless. Moreover, we did not consider where the rank of the sepsis code within the ICD-9 sequence for each individual patient. For example, we considered a patient with a sepsis code in the #1 slot (indicating a primary diagnosis) and the #10 slot to be the same. Future studies could use prior weights based on this ICD-9 code sequence or avoid ICD-9 codes altogether and determine a physiological cutoff for the onset of sepsis.

## Conclusions

In this project, we have shown that a) supervised classification techniques b) summary statistics for routine, non-invasive clinical features and c) imputation methods can be implemented in order to predict sepsis in the ICU, albeit only marginally better than current systems. Logistic regression - after feature elimination based on collinearity - provided the best performance while maintaining interpretability. This work presented in this report may serve to guide the development of an early warning system for sepsis in the intensive care setting. 

# Acknowledgements

Thanks Noemie, Andrew, and William for making our last semester at CUMC a blast! And for making access to the MIMIC II database a lot more tolerable than it otherwise would have been.

# Session Information

```{r session_info}
sessionInfo()
```

# References
